{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Build_and_Deploy_Data_Science_Products_.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Projects-/blob/master/Build_and_Deploy_Data_Science_Products_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wJEe-T6XuIZ"
      },
      "source": [
        "# **1-Understand the landscape of solutions available for machine translation**[1]\n",
        "\n",
        "\n",
        "In this series we will take a use case, understand the solution landscape and its evolution, explore different architecture choices, look under the hood of the architecture to understand the nuts and bolts, build a prototype, convert the prototype into production ready code, build an application from the production ready code and finally understand the process for deploying the application.\n",
        "\n",
        "The use case we will be dealing with will be **Machine Translation**. By the end of the series you would have working knowledge on how to build and deploy a Machine translation application, which translates, German sentences into English. This series will comprise of the following posts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-opydTdY3Hq"
      },
      "source": [
        "## **1.1 Introduction to Machine Translation**[1]\n",
        "\n",
        "**language translation** has always been a tough nut to crack. What makes it tough is the variations in structure and lexicon when one traverses from one language to the other. For this reason the problem of automated language translation or Machine translation has fascinated and inspired the best minds. Over the past decade some trailblazing advances have happened within this field. We have now reached a stage where machine translation has become quite ubiquitous. These technologies are now embedded in all our devices, mobiles, watches, desktops, tablets etc and have become an integral part of our every day life. A common example is the **Google Translate service** which has the capability to identify our input languge and subsequently translate it to multitudes of languages.\n",
        "\n",
        "**Machine translation** technologies have transcended different approaches before reaching the state we are in at present. Let us take a quick look at the evolution of the solution landscape of machine translation.\n",
        "\n",
        "The journey to the current state of the art translation technologies tells a fascinating tale of the strides in machine learning.\n",
        "\n",
        "The evolution of machine translation can be demarcated to three distinct phase. Let us look at each one of them and understand its distinct characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kJFQyMBakvx"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=16oSEPChc0lXTMMBufQF2noWwEi2puYsA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIL_QswGt_O_"
      },
      "source": [
        "## **1.2 Classical Machine Translation [1]**\n",
        "Classical machine translation methods relies heavily on linguisitc rules and deep domain knowledge to translate from a source language to a target language. There are three approaches under this method.\n",
        "\n",
        "**Direct Translation**\n",
        "\n",
        "Direct translation is based on a large bilingual dictionary;each entry in the dictionary can be viewed as a small program whose job is to translate one word”\n",
        "\n",
        "As the name suggests this method adopts a word-to-word translation of the source language to the target language. After the word to word translation a re-ordering of the translated words are required based on linguistic rules formulated between the source language and target language.\n",
        "\n",
        "**Transfer Method**[1]\n",
        "\n",
        "In the example we saw on direct translation method, we saw how the mapping of the English words for the translated Spanish sentence had a complete different ordering from the source English sentence. Every language has such structural charachteristics inherent in them. Transfer methods looks at tapping the structural differences between different language pairs.\n",
        "\n",
        "Unlike the direct method where there is word to word tranlation followed by re-ordering, transfer methods relies on codification of the contrastive knowledge i.e difference between languages, for translation from the source to the target language. Similar to the direct method, this method also relies on deep domain knowledge and codification of complex rules governing language construction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQnW_lrqRL8s"
      },
      "source": [
        "**Interlingua Method [1]**\r\n",
        "\r\n",
        "The intelingua method works on a completely different approach to the word to word and contrastive translations methods we have already seen.\r\n",
        "\r\n",
        "The intelingua method resonates very closely to the process by which human translators work. When translating , a human translator understands the meaning of the source sentence and translate it to the target language so that the essence of the conversation is not lost. There might not be a word to word mapping of the source sentence and translated sentence. However the meaning would remain intact. This is the principle adopted in the intelingua methods. Like the other two methods in the classical approach, intelingua method also depends on the rich codification of rules and dictionaries\r\n",
        "\r\n",
        "The classical machine translation methods were effective for a large set of use cases. However the classical methods relied on comprehensive set of rules and large dictionaries. Building such knowledge base was a mammoth task requiring specialised skills and expertise. The complexity increased many fold when designing systems able to handle translation of multiple languages. There was a need for an approach different from the domain intensive classical techniques. This led to the rise in popularity of the statistical methods in machine translation.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7BOTpH8SGcq"
      },
      "source": [
        "**Statistical Machine Translation [1]**\r\n",
        "\r\n",
        "When we explored the classical methods we understood the over dependence on domain knowledge in creating linguistic rules and dictionaries. However it was also a fact that no amount of domain knowledge was enough to handle the intricate nuances of languages. What if phrases, idioms and specialised usages in a language do not have any parallels in another language ? In such circumstances what a linguist would do is to go for the closest match given the source language.\r\n",
        "\r\n",
        "This idea of selecting the most probable sentence in the target languge given a sentence in source language is what is leaveraged in statistical machine translation.\r\n",
        "\r\n",
        "Statistical methods builds probabilistic models that aims at maximizing the probability of the target sentence which best captures the essence of the source sentence. In probability terms we can represent this as\r\n",
        "\r\n",
        "argmaxT P(T|S)\r\n",
        "\r\n",
        "where T and S are the target and source languages respectively. The above form is the representation of a posterior probability as per Bayes Theorm. This is proportional to\r\n",
        "\r\n",
        "= argmaxT P(S|T) * P(T)\r\n",
        "\r\n",
        "The first term ( P(S|T) ) is called the translation model and can be interpreted as the likelihood of finding the source sentence given the target sentence. The second term P(T) is called the language model which represents the conditional probability of a word in the languge given some preceeding words.\r\n",
        "\r\n",
        "The statistical model aims at finding the conditional probabilities of words within a corpora and using these probabilities find the best possible translation. Statistical machine translation models make use of large corpora or text available on the source and target languages. Eventhough statistical methods were effective, they also had some weaknesses. This method was predominantly focussed on phrases being translated thereby compromising the broder context of the target language. This method struggled when required to translate to a target language which was different in context from the source context. These shortcomings paved the way to advances in other methods which were more robust to retaining the context between the source and target languages.\r\n",
        "\r\n",
        "**Neural Machine Translation**\r\n",
        "\r\n",
        "Neural machine translation is a different approach where artifical neural networks are used for machine translation. In the statistical machine translation approaches we saw that it uses multiple components like the translation model and language model to do the translations.In NMT models the entire sentence is a single integrated model. In term of approach there isnt drastic deviations from the statistical approaches. However NMTs uses vector representations of words and sentences, which helps in retaining the context of the source and target sentences.\r\n",
        "\r\n",
        "There are different approaches for machine translation using artificial neural networks. One of the earlier approach was to use a multi layer perceptron or a fully connected network for machine translation. However these models werent effective for large sequences of sentences.\r\n",
        "\r\n",
        "Many shortfalls of the earlier approaches were addressed by the adoption of Recurrent Neural network models (RNNs) for machine translation. RNNs are those class of neural networks suited for sequence data. Languages as you know are manifestations of sequence of words with interdependencies between the words within the sequence. RNNs are capable of handling such interdependencies which made such class of models more suited for machine translation. There are different variations of Sequence models which are used for machine translation like encoder-decoder, encoder-decoder with attention etc. We will be using the encoder-decoder models for building our application and will be dealt with in greater depth in the next post.\r\n",
        "\r\n",
        "The state of the art models for machine translation currently are the Transformer models. Transformer models make use of the concept of attention and then builds on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03xaqcF2hK_I"
      },
      "source": [
        "# **2- II : Build and Deploy Data Science Products : Exploring Sequence to Sequence architecture for Machine Translation.**\r\n",
        "\r\n",
        "We already know that the problem of machine translation entails deciphering sequence of words in a source language to predict a sequence of target language. For example if you look at the following input German sequence\r\n",
        "\r\n",
        "Ich freue mich darauf, etwas über maschinelle Übersetzung zu lernen.\r\n",
        "Which can be translated to \r\n",
        "\r\n",
        "I look forward to learning about machine translation\r\n",
        "From these sequences we can observe the following.\r\n",
        "\r\n",
        "The length of input sequence and the length of the target sequence are different\r\n",
        "There is no one to one mapping between words from the input language to the target language\r\n",
        "There is dependence on the context which needs to be learned from the input language to get the best translation for the target language.\r\n",
        "The inherent complexities like these in machine translation made models like multi layer perceptron ineffective for machine translation. The need of the hour was a model architecuture which was capable of looking accross sequences of words and understand the context of the source language to effectively translate to the target language. This is where Recurrent Neural Networks (RNNs) became popular for solving machine translation problems. Let us now take a deeper look at RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc92Iobbx--m"
      },
      "source": [
        "## **2.1 Recurrent Neural Networks ( RNNs)**\r\n",
        "\r\n",
        "RNN models which fall under the category of Sequence to sequence models are designed to learn the context of any input language. But why is learning the context important ? Let us understand this with a simple example.\r\n",
        "\r\n",
        "Suppose we are predicting the next character in a sequence for the string “Happy B….”. We need to predict the next character after the letter ‘B’. For the time being let us assume that we are ignoring the word “Happy” falling before the letter B. In such a scenario the best bet would be to look for all the words which start with “B” and choose the word which is most frequent. Let us say the most frequent word starting with “B” is the word “Baby”. So the next character which will be predicted would be the letter “a”. Now let us imagine that we started looking at all the characters which preceeds B. Given the information about the preceeding charachters “H”,”A”,”P”,”P”,”Y” “B”, then the probability of predicting ‘i’ would be the highest since the word “Birthday” is the most likely word given the context “Happy B” . This is where the concept of context becomes very significant. Language translation depends a lot on the context and therefore there was the need to adopt an architecture where context was learned. Sequence to sequence models like RNNs became an obvious choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY26eEB02FV8"
      },
      "source": [
        "![](\r\n",
        "https://drive.google.com/uc?export=view&id=1iArA19WCN9R0IAG_tUssJcinl-O-qhCA)\r\n",
        "\r\n",
        "The dynamics of RNN can be represented as above. The circular nodes represents each time step in the sequence. Each of the time steps receives an input represetend as the arrow pointing upwards. In this context each letter in the string becomes the input at each time step. With each character input the output or the prediction is represented at the top. So given the letter ‘H’ the prediction is the letter ‘A’. Once the letter ‘A’ is predicted it becomes the next input and we need to predict the next letter given the context that we had the letter ‘H’ at the previous time step. At each time step we can also see that there is an arrow which points to the right. This is the information or context each time step passes on to the subsequent time step enabling it to predict contextually.\r\n",
        "\r\n",
        "Unlike vanilla neural networks where each layer has a set of parameters, RNNs shares the same parameters accross all the time steps. Because the parameters are shared accross all time steps, the implementation of back propogation is a little different for the case of RNNs. The type of back propogation implemented in RNN is called Back propogation through time(BPTT). We will be covering the dynamics of BPTT with a toy example in the fourth blog of this series.\r\n",
        "\r\n",
        "Earlier we saw that the RNN keeps the context of the previous time steps in memory and applies it when predicting for the time step in consideration. However in practice vanilla RNNs fails when it encounters large sequences. The parameters blow up or shrink to very small values in such cases. These scenarios are called exploding gradients and vanishing gradients respectively. So in practice a RNN can only leaverage few time steps to extract the context. To over come these shortcomings different variations sequence to sequence models are used. One such variation is the LSTM Long Short Term Memory network. We will be using the LSTM network in our application for machine translation. Let us first look at what an LSTM looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynoCkBlk2miz"
      },
      "source": [
        "## **2.3 Long Short Term Memory Network ( LSTM)**\r\n",
        "\r\n",
        "LSTMs, like vanialla RNNs, have the recurrent connections which entails that the context from the previous time steps are passed on to the current time step when generating an output. However we discussed in the previous section on RNN that they suffer from a major problem of exploding or vanishing gradients when encountered with long sequences. This shortcoming was overcome by building a memory block in LSTMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOt5DCp5XdD6"
      },
      "source": [
        "# **References**\n",
        "\n",
        "[[1] Build and Deploy Data Science Products : A Practical Guide to Building a Machine Translation Application.](https://bayesianquest.com/2020/10/24/build-your-machine-translation-application-byte-by-byte/)\n",
        "\n",
        "[[2] II:Build and Deploy Data Science Products : Exploring Sequence to Sequence architecture for Machine Translation.](https://bayesianquest.com/2020/10/24/ii-build-and-deploy-data-science-products-exploring-sequence-to-sequence-architecture-for-machine-translation/)"
      ]
    }
  ]
}