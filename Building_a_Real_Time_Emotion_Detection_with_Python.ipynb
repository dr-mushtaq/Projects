{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Building a Real Time Emotion Detection with Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Projects-/blob/master/Building_a_Real_Time_Emotion_Detection_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZFQQORCAcS",
        "colab_type": "text"
      },
      "source": [
        "# Building a Real Time Emotion Detection with Python\n",
        "# **Introduction:**\n",
        "\n",
        "Detecting real-time emotion of the person with a camera input is one of the advanced features in the machine learning process. The detection of emotion of a person using a camera is useful for various research and analytics purposes. The detection of emotion is made by using the machine learning concept. You can use the trained dataset to detect the emotion of the human being. For detecting the different emotions, first you need to train those different emotions, or you can use a dataset already available on the internet. In this article, we will discuss creating a Python program to detect real-time emotion of a human being using the camera '[1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAVZc4ayyDWL",
        "colab_type": "text"
      },
      "source": [
        "# 1-Installing Dependencies "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpWN1sJv8eho",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b09ba454-dd8b-44b1-91ea-8b165d079a43"
      },
      "source": [
        "!pip install opencv-python"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWA9EJ-zwyej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensor flow "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6X6cfXcw4uL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a7e31d5-f028-4e75-daae-b4095ecc7eb9"
      },
      "source": [
        "!pip install numpy "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le9Nov-7w7X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pandas  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFMnGbhH9sIS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2d55369a-2c7f-40eb-e502-b4516aa344c4"
      },
      "source": [
        "!pip install keras  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Pia2KM9x9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "909580b3-a1ab-43ce-d289-fb6126ec8558"
      },
      "source": [
        "!pip install adam  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adam\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/80/f822a29a54098e22cee0131fa67ad4902106e576c5096e12a7bb11845f16/adam-0.0.0.dev0-py2.py3-none-any.whl\n",
            "Installing collected packages: adam\n",
            "Successfully installed adam-0.0.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnF8AVoy93tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kwargs  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy8GBKBY99lX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install cinit  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnY7VuCDI-qN",
        "colab_type": "text"
      },
      "source": [
        "# 1 **Training the Dataset**#\n",
        "\n",
        "For training purposes, I use the predefined un trained dataset CSV file as my main input for my input for training the machine. You can use the code given below for training the machine using the dataset. Before that, you need to ensure that all required files in the same repository where the program presents otherwise it will through some error. You can download the data set by clicking here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nhYP15aoCAcf",
        "colab_type": "text"
      },
      "source": [
        "# 2 - **Import library** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1wsGi52CAch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os  \n",
        "import pandas as pd  \n",
        "import numpy as np "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfWGnlhx_GYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "435310a0-47bb-4d7a-c647-260502fa72e4"
      },
      "source": [
        "from keras.models import Sequential  \n",
        "from keras.layers import Dense, Dropout, Activation, Flatten  \n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D  \n",
        "from keras.losses import categorical_crossentropy  \n",
        "from keras.optimizers import Adam  \n",
        "from keras.regularizers import l2  \n",
        "from keras.utils import np_utils  \n",
        "# pd.set_option('display.max_rows', 500)  \n",
        "# pd.set_option('display.max_columns', 500)  \n",
        "# pd.set_option('display.width', 1000)  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XQFJb0YzEYO",
        "colab_type": "text"
      },
      "source": [
        "#2 - **Load Dataset** # \n",
        "The both training and evaluation operations would be handled with Fec2013 dataset. Compressed version of the dataset takes 92 MB space whereas uncompressed version takes 295 MB space. There are 28K training and 3K testing images in the dataset. Each image was stored as 48×48 pixel. The pure dataset consists of image pixels (48×48=2304 values), emotion of each image and usage type (as train or test instance)[2]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hncBMFUXFN6G",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c954b9e2-5a10-4c05-8219-3769da293b8a"
      },
      "source": [
        "# this code is used to upload dataset from Pc to colab\n",
        "from google.colab import files # Please First run this cod in chrom \n",
        "def getLocalFiles():\n",
        "    _files = files.upload() # upload StudentNextSessionf.csv datase\n",
        "    if len(_files) >0: # Then run above  libray \n",
        "       for k,v in _files.items():\n",
        "         open(k,'wb').write(v)\n",
        "getLocalFiles()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-02c058da-bda4-4f39-8e0d-ec1fb2d3cded\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-02c058da-bda4-4f39-8e0d-ec1fb2d3cded\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving angry.jpg to angry.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPlEL8_ADW3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -N https://www.kaggle.com/deadskull7/fer2013"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqD8iiJZzTZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('fer2013.csv') "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpHsrVexXuWj",
        "colab_type": "text"
      },
      "source": [
        "#2-Data Description "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia4AAHlC6Mhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df.info())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS0gKFmiHAvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df[\"Usage\"].value_counts())  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zimrlUU8HEy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4QYI2IoHWpL",
        "colab_type": "text"
      },
      "source": [
        "# 3- **Data Spliting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9JGZRKCCAcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,train_y,X_test,test_y=[],[],[],[]  \n",
        "\n",
        "for index, row in df.iterrows():  \n",
        "    val=row['pixels'].split(\" \")  \n",
        "    try:  \n",
        "        if 'Training' in row['Usage']:  \n",
        "           X_train.append(np.array(val,'float32'))  \n",
        "           train_y.append(row['emotion'])  \n",
        "        elif 'PublicTest' in row['Usage']:  \n",
        "           X_test.append(np.array(val,'float32'))  \n",
        "           test_y.append(row['emotion'])  \n",
        "    except:  \n",
        "        print(f\"error occured at index :{index} and row:{row}\") "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6333A5woSWX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 64  \n",
        "num_labels = 7  \n",
        "batch_size = 64  \n",
        "epochs = 30  \n",
        "width, height = 48, 48"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsFBKUfkSYeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.array(X_train,'float32')  \n",
        "train_y = np.array(train_y,'float32')  \n",
        "X_test = np.array(X_test,'float32')  \n",
        "test_y = np.array(test_y,'float32')  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcTmmZATSh5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y=np_utils.to_categorical(train_y, num_classes=num_labels)  \n",
        "test_y=np_utils.to_categorical(test_y, num_classes=num_labels) "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehTTQE9YJF1Q",
        "colab_type": "text"
      },
      "source": [
        "#4- **Normalizing data between 0 and 1** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TvqFi8-XCAcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train -= np.mean(X_train, axis=0)  \n",
        "X_train /= np.std(X_train, axis=0)  \n",
        "X_test -= np.mean(X_test, axis=0)  \n",
        "X_test /= np.std(X_test, axis=0)  \n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)  \n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izbM6eEUKAV-",
        "colab_type": "text"
      },
      "source": [
        "# 5- **Designing the CNN**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrgUo8fTbXz",
        "colab_type": "text"
      },
      "source": [
        "##5.1- **1st convolution layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt-vctO1Kb3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()  \n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))  \n",
        "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Dropout(0.5))  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x8QNZN3CAdF",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 - **2nd Convolution Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBdXxiHLfLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Dropout(0.5)) "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLfUeKr2UNe7",
        "colab_type": "text"
      },
      "source": [
        "## 5.3- **3rd Convolution Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUJ16arHaKMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))  \n",
        "# model.add(BatchNormalization())  \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))  \n",
        "model.add(Flatten())  "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7EabEFjUl6t",
        "colab_type": "text"
      },
      "source": [
        "##5.4-  **Fully connected neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_XHGn_oUvMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(1024, activation='relu'))  \n",
        "model.add(Dropout(0.2))  \n",
        "model.add(Dense(1024, activation='relu'))  \n",
        "model.add(Dropout(0.2))  \n",
        "model.add(Dense(num_labels, activation='softmax'))  \n",
        "# model.summary()  "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOCxPs8sU5aP",
        "colab_type": "text"
      },
      "source": [
        "#6-**Compliling the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi-DFb_OVBr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=categorical_crossentropy,  \n",
        "              optimizer=Adam(),  \n",
        "              metrics=['accuracy'])  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSM9ZdxfVIxE",
        "colab_type": "text"
      },
      "source": [
        "#7-**Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02q29lEXVRjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e5c36ce-4b67-4df5-ab3f-29a96ec58801"
      },
      "source": [
        "model.fit(X_train, train_y,  \n",
        "          batch_size=batch_size,  \n",
        "          epochs=epochs,  \n",
        "          verbose=1,  \n",
        "          validation_data=(X_test, test_y),  \n",
        "          shuffle=True)  "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/30\n",
            "28709/28709 [==============================] - 17s 599us/step - loss: 1.7047 - accuracy: 0.3030 - val_loss: 1.5874 - val_accuracy: 0.3658\n",
            "Epoch 2/30\n",
            "28709/28709 [==============================] - 9s 326us/step - loss: 1.5055 - accuracy: 0.4105 - val_loss: 1.3826 - val_accuracy: 0.4636\n",
            "Epoch 3/30\n",
            "28709/28709 [==============================] - 9s 326us/step - loss: 1.3942 - accuracy: 0.4616 - val_loss: 1.3233 - val_accuracy: 0.4840\n",
            "Epoch 4/30\n",
            "28709/28709 [==============================] - 9s 330us/step - loss: 1.3324 - accuracy: 0.4850 - val_loss: 1.2805 - val_accuracy: 0.5141\n",
            "Epoch 5/30\n",
            "28709/28709 [==============================] - 9s 330us/step - loss: 1.2844 - accuracy: 0.5038 - val_loss: 1.2352 - val_accuracy: 0.5241\n",
            "Epoch 6/30\n",
            "28709/28709 [==============================] - 9s 331us/step - loss: 1.2452 - accuracy: 0.5218 - val_loss: 1.2353 - val_accuracy: 0.5183\n",
            "Epoch 7/30\n",
            "28709/28709 [==============================] - 10s 332us/step - loss: 1.2135 - accuracy: 0.5351 - val_loss: 1.2107 - val_accuracy: 0.5372\n",
            "Epoch 8/30\n",
            "28709/28709 [==============================] - 9s 330us/step - loss: 1.1852 - accuracy: 0.5475 - val_loss: 1.1864 - val_accuracy: 0.5433\n",
            "Epoch 9/30\n",
            "28709/28709 [==============================] - 10s 333us/step - loss: 1.1597 - accuracy: 0.5563 - val_loss: 1.1915 - val_accuracy: 0.5433\n",
            "Epoch 10/30\n",
            "28709/28709 [==============================] - 10s 334us/step - loss: 1.1355 - accuracy: 0.5663 - val_loss: 1.1832 - val_accuracy: 0.5492\n",
            "Epoch 11/30\n",
            "28709/28709 [==============================] - 10s 333us/step - loss: 1.1116 - accuracy: 0.5724 - val_loss: 1.1664 - val_accuracy: 0.5508\n",
            "Epoch 12/30\n",
            "28709/28709 [==============================] - 10s 332us/step - loss: 1.0945 - accuracy: 0.5793 - val_loss: 1.1790 - val_accuracy: 0.5517\n",
            "Epoch 13/30\n",
            "28709/28709 [==============================] - 10s 333us/step - loss: 1.0812 - accuracy: 0.5863 - val_loss: 1.1556 - val_accuracy: 0.5612\n",
            "Epoch 14/30\n",
            "28709/28709 [==============================] - 10s 332us/step - loss: 1.0534 - accuracy: 0.5952 - val_loss: 1.1517 - val_accuracy: 0.5614\n",
            "Epoch 15/30\n",
            "28709/28709 [==============================] - 10s 332us/step - loss: 1.0447 - accuracy: 0.6006 - val_loss: 1.1668 - val_accuracy: 0.5481\n",
            "Epoch 16/30\n",
            "28709/28709 [==============================] - 10s 333us/step - loss: 1.0261 - accuracy: 0.6066 - val_loss: 1.1646 - val_accuracy: 0.5648\n",
            "Epoch 17/30\n",
            "28709/28709 [==============================] - 10s 336us/step - loss: 1.0065 - accuracy: 0.6157 - val_loss: 1.1556 - val_accuracy: 0.5639\n",
            "Epoch 18/30\n",
            "28709/28709 [==============================] - 10s 333us/step - loss: 0.9892 - accuracy: 0.6174 - val_loss: 1.1454 - val_accuracy: 0.5651\n",
            "Epoch 19/30\n",
            "28709/28709 [==============================] - 10s 334us/step - loss: 0.9723 - accuracy: 0.6291 - val_loss: 1.1765 - val_accuracy: 0.5687\n",
            "Epoch 20/30\n",
            "28709/28709 [==============================] - 10s 336us/step - loss: 0.9575 - accuracy: 0.6363 - val_loss: 1.1790 - val_accuracy: 0.5690\n",
            "Epoch 21/30\n",
            "28709/28709 [==============================] - 10s 333us/step - loss: 0.9385 - accuracy: 0.6419 - val_loss: 1.1906 - val_accuracy: 0.5704\n",
            "Epoch 22/30\n",
            "28709/28709 [==============================] - 10s 333us/step - loss: 0.9297 - accuracy: 0.6460 - val_loss: 1.1913 - val_accuracy: 0.5773\n",
            "Epoch 23/30\n",
            "28709/28709 [==============================] - 10s 339us/step - loss: 0.9132 - accuracy: 0.6493 - val_loss: 1.1747 - val_accuracy: 0.5704\n",
            "Epoch 24/30\n",
            "28709/28709 [==============================] - 10s 341us/step - loss: 0.8888 - accuracy: 0.6604 - val_loss: 1.2045 - val_accuracy: 0.5659\n",
            "Epoch 25/30\n",
            "28709/28709 [==============================] - 10s 332us/step - loss: 0.8806 - accuracy: 0.6649 - val_loss: 1.2035 - val_accuracy: 0.5598\n",
            "Epoch 26/30\n",
            "28709/28709 [==============================] - 10s 332us/step - loss: 0.8578 - accuracy: 0.6713 - val_loss: 1.2296 - val_accuracy: 0.5589\n",
            "Epoch 27/30\n",
            "28709/28709 [==============================] - 10s 336us/step - loss: 0.8436 - accuracy: 0.6786 - val_loss: 1.2276 - val_accuracy: 0.5701\n",
            "Epoch 28/30\n",
            "28709/28709 [==============================] - 10s 333us/step - loss: 0.8357 - accuracy: 0.6829 - val_loss: 1.2247 - val_accuracy: 0.5795\n",
            "Epoch 29/30\n",
            "28709/28709 [==============================] - 10s 334us/step - loss: 0.8288 - accuracy: 0.6858 - val_loss: 1.2309 - val_accuracy: 0.5779\n",
            "Epoch 30/30\n",
            "28709/28709 [==============================] - 10s 335us/step - loss: 0.8119 - accuracy: 0.6925 - val_loss: 1.2497 - val_accuracy: 0.5712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f6d34622f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKCwzVMnVaOm",
        "colab_type": "text"
      },
      "source": [
        "#8-**Saving the  model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-PUeUv-VglW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fer_json = model.to_json()  \n",
        "with open(\"fer.json\", \"w\") as json_file:  \n",
        "    json_file.write(fer_json)  \n",
        "model.save_weights(\"fer.h5\") "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtpS-joPfJqi",
        "colab_type": "text"
      },
      "source": [
        "#9-**Evaluate model [2]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S05tU0VfhVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_score = model.evaluate(X_train, train_y, verbose=0)\n",
        "print('Train loss:', train_score[0])\n",
        "print('Train accuracy:', 100*train_score[1])\n",
        "test_score = model.evaluate(X_test, test_y, verbose=0)\n",
        "print('Test loss:', test_score[0])\n",
        "print('Test accuracy:', 100*test_score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UC9kS_Rv_hA",
        "colab_type": "text"
      },
      "source": [
        "# **10-Confusion Matrix**[2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmsbOnwbwJTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        " \n",
        "pred_list = []; actual_list = []\n",
        " \n",
        "for i in predictions:\n",
        " \n",
        "pred_list.append(np.argmax(i))\n",
        " \n",
        "for i in y_test:\n",
        " \n",
        "actual_list.append(np.argmax(i))\n",
        " \n",
        "confusion_matrix(actual_list, pred_list)\n",
        "Testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8WWRWP0wh2Z",
        "colab_type": "text"
      },
      "source": [
        "#**11-Testing**[2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjDLBzNZw7Tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from matplotlib import pyplot as plt\n",
        "img = image.load_img(\"angry.jpg\", grayscale=True, target_size=(48, 48))\n",
        " \n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis = 0)\n",
        " \n",
        "x /= 255\n",
        " \n",
        "custom = model.predict(x)\n",
        "emotion_analysis(custom[0])\n",
        " \n",
        "x = np.array(x, 'float32')\n",
        "x = x.reshape([48, 48]);\n",
        " \n",
        "plt.gray()\n",
        "plt.imshow(x)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Te0y5lxDan",
        "colab_type": "text"
      },
      "source": [
        "Emotions stored as numerical as labeled from 0 to 6. Keras would produce an output array including these 7 different emotion scores. We can visualize each prediction as bar chart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyQL0kDKwyxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def emotion_analysis(emotions):\n",
        "  objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "  y_pos = np.arange(len(objects))\n",
        "  plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
        "  plt.xticks(y_pos, objects)\n",
        "  plt.ylabel('percentage')\n",
        "  plt.title('emotion')\n",
        "  plt.show()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNk8Y4m5b3tf",
        "colab_type": "text"
      },
      "source": [
        "#9-Detecting Real-Time Emotion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgAXg8iCb88r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os  \n",
        "import cv2  \n",
        "import numpy as np  \n",
        "from keras.models import model_from_json  \n",
        "from keras.preprocessing import image  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJbt4FopcEyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load model  \n",
        "model = model_from_json(open(\"fer.json\", \"r\").read())  \n",
        "#load weights  \n",
        "model.load_weights('fer.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb9gAA3fcMGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srFH4QN7cNW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap=cv2.VideoCapture(0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxitWGrYcUIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap=cv2.VideoCapture(0)  \n",
        "while True:  \n",
        "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image  \n",
        "    if not ret:  \n",
        "        continue  \n",
        "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)  \n",
        "\n",
        "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)  \n",
        "\n",
        "\n",
        "    for (x,y,w,h) in faces_detected:  \n",
        "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)  \n",
        "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n",
        "        roi_gray=cv2.resize(roi_gray,(48,48))  \n",
        "        img_pixels = image.img_to_array(roi_gray)  \n",
        "        img_pixels = np.expand_dims(img_pixels, axis = 0)  \n",
        "        img_pixels /= 255  \n",
        "\n",
        "        predictions = model.predict(img_pixels)  \n",
        "\n",
        "        #find max indexed array  \n",
        "        max_index = np.argmax(predictions[0])  \n",
        "\n",
        "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')  \n",
        "        predicted_emotion = emotions[max_index]  \n",
        "\n",
        "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)  \n",
        "\n",
        "    resized_img = cv2.resize(test_img, (1000, 700))  \n",
        "    cv2.imshow('Facial emotion analysis ',resized_img)  \n",
        "\n",
        "\n",
        "\n",
        "    if cv2.waitKey(10) == ord('q'):#wait until 'q' key is pressed  \n",
        "        break  \n",
        "\n",
        "cap.release()  \n",
        "cv2.destroyAllWindows  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7uTbEWCAei",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "\n",
        "[1] Building a Real Time Emotion Detection with Python\n",
        "\n",
        "https://morioh.com/p/801c509dda99?f=5c21f93bc16e2556b555ab2f\n",
        "\n",
        "[2] Facial Expression Recognition with Keras\n",
        "http://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/\n",
        "\n"
      ]
    }
  ]
}